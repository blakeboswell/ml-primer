intercept + slope * 7.25
intercept + slope * 6.25
intercept + slope * 6.75
intercept + slope * 6.70
intercept + slope * 6.72
intercept + slope * 6.71
intercept + slope * 6.715
intercept + slope * 6.72
intercept + slope * 6.715
intercept + slope * 6.718
intercept + slope * 6.717
py
order(py)
py[-order(py)]
## logistic function
g <- function(z) 1 / (1 + exp(-z))
## output logistic plot
library(jsonlite)
z <- seq(-5, 5, 0.1)
toJSON(g(z))
toJSON(z)
## vectorized logistic gradient function
logistic.gradient <- function(x, y, theta){
t(x) %*% (1 / (1 + exp(-(x %*% theta))) - y)
}
## gradient descent closure
gradient.descent <- function(gradient){
grad <- gradient
fit <- function(x, y, theta = NULL, maxiter = 10000, alpha = 0.05, tol = .00001){
x <- as.matrix(x)
y <- as.matrix(y)
m <- ncol(x)
if(is.null(theta)) theta <- matrix(0, nrow = m, 1)
for(i in 1:maxiter){
theta.tmp <- theta
theta <- theta.tmp - alpha * grad(x, y, theta.tmp)
}
return(
list(theta = theta)
)
}
return(fit)
}
library(dplyr)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Sepal.Length, Sepal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
## use gradient descent to fit parameter
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x, y)$theta
## generate predictions using the best fit theta
py <- g(as.matrix(x) %*% theta_star)
## apply decision rule to assign to 0,1 label
yhat <- ifelse(py < 0.5, 0, 1)
## test to see how well the data fits
sum(yhat == y) / nrow(y)
## check out the coefficients
theta_star
toJSON(x[yhat == 1, 2])
toJSON(x[yhat != 1, 2])
toJSON(x[yhat == 1, 3])
toJSON(x[yhat != 1, 3])
## calculate linear coefficients
slope <- -theta_star[2] / theta_star[3]
intercept <- -theta_star[1] / theta_star[3]
plot(py)
names(cars)
names(mtcars)
head(names(mtcars)
)
head(mtcars)
class(mtcars$vs)
x <- mtcars %>% mutate(intercept = 1) %>%
select(intercept, hp, mpg) %>% as.matrix
y <- mtcars %>% select(vs)
theta <- mdl(x,y)$theta
theta
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
x <- mtcars %>% mutate(intercept = 1) %>%
select(intercept, hp, mpg) %>% as.matrix
y <- mtcars %>% select(vs)
theta <- mdl(x,y)$theta
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
plot(x)
plot(x[,2], x[,3])
mtcars %>% select(cyl, disp )
mtcars %>% select(cyl, disp ) %>% plot
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
mydat %>% select(gre, gpa) %>% plot
mydata %>% select(gre, gpa) %>% plot
mydata %>% select(gre, gpa, admit) %>% plot(col = admit)
mydata %>% select(gre, gpa, admit) %>% plot(col = .admit)
mydata %>% select(gre, gpa) %>% plot(col = admit)
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
x <- mydata %>% mutate(intercept = 1) %>%
select(intercept, cyl, disp) %>% as.matrix
y <- mydata %>% select(admit)
theta <- mdl(x,y)$theta
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
x <- mydata %>% mutate(intercept = 1) %>%
select(intercept, gre, gpa) %>% as.matrix
y <- mydata %>% select(admit)
theta <- mdl(x,y)$theta
theta
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
x <- mydata %>% mutate(intercept = 1) %>%
select(intercept, gre, gpa, rank) %>% as.matrix
y <- mydata %>% select(admit)
theta <- mdl(x,y)$theta
theta
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
plot(x[,2], x[,3])
plot(x[,2], x[,3], col = y)
plot(x[,2], x[,3], col = yhat)
plot(x[,2], x[,3], col = ifelse(yhat == 1, 1, 2))
plot(x[,1], x[,3], col = ifelse(yhat == 1, 1, 2))
library(dplyr)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x, y)$theta
py <- g(as.matrix(x) %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
theta_star
plot(yhat)
plot(py)
library(dplyr)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Sepal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x, y)$theta
py <- g(as.matrix(x) %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
plot(py)
theta_star
library(dplyr)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x, y)$theta
py <- g(as.matrix(x) %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
plot(py)
train_i <- sample_n(100)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% sample_n(m) %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% filter(train_ind) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% filter(train_ind) %>% as.matrix
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% sample_n(m) %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% slice(train_ind) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% slice(train_ind) %>% as.matrix
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x, y)$theta
nrow(iris)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% sample_n(m) %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% slice(train_ind) %>% as.matrix
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% slice(train_ind) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% slice(train_ind) %>% as.matrix
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x, y)$theta
py <- g(as.matrix(x) %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
## use gradient descent to fit parameter
mdl <- gradient.descent(logistic.gradient)
theta_star <- mdl(x[train_ind, ], y[train_ind, ])$theta
## generate predictions using the best fit theta
py <- g(as.matrix(x[-train_ind, ]) %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / nrow(y)
sum(yhat == y[-train_ind, ]) / nrow(y)
theta_star
slope <- -theta_star[2] / theta_star[3]
intercept <- -theta_star[1] / theta_star[3]
plot(py)
plot(py, col = ifelse(yhat == 1, 2, 4))
plot(py, col = ifelse(y[-train_ind, ] == 1, 2, 4))
sum(yhat == y[-train_ind, ]) / nrow(y[-train_ind, ])
sum(yhat == y[-train_ind, ]) / nrow(y[-train_ind, ])
sum(yhat == y[-train_ind, ]) / lengthy([-train_ind, ])
sum(yhat == y[-train_ind, ]) / lengthy(y[-train_ind, ])
sum(yhat == y[-train_ind, ]) / length(y[-train_ind, ])
py <- g(x[-train_ind, ] %*% theta_star)
## logistic function
g <- function(z) 1 / (1 + exp(-z))
## output logistic plot
library(jsonlite)
z <- seq(-5, 5, 0.1)
toJSON(g(z))
toJSON(z)
## vectorized logistic gradient function
logistic.gradient <- function(x, y, theta){
t(x) %*% (1 / (1 + exp(-(x %*% theta))) - y)
}
## gradient descent closure
gradient.descent <- function(gradient){
grad <- gradient
fit <- function(x, y, theta = NULL, maxiter = 10000, alpha = 0.05, tol = .00001){
x <- as.matrix(x)
y <- as.matrix(y)
m <- ncol(x)
if(is.null(theta)) theta <- matrix(0, nrow = m, 1)
for(i in 1:maxiter){
theta.tmp <- theta
theta <- theta.tmp - alpha * grad(x, y, theta.tmp)
}
return(
list(theta = theta)
)
}
return(fit)
}
## initialize a gradient descent closure with the logistic gradient
mdl <- gradient.descent(logistic.gradient)
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
## fit theta to the training set
theta_star <- mdl(x[train_ind, ], y[train_ind, ])$theta
## generate predictions using the best fit theta
py <- g(x[-train_ind, ] %*% theta_star)
## apply decision rule to assign to 0,1 label
yhat <- ifelse(py < 0.5, 0, 1)
## test to see how well the data fits
sum(yhat == y[-train_ind, ]) / length(y[-train_ind, ])
## check out the coefficients
theta_star
toJSON(x[yhat == 1, 2])
toJSON(x[yhat != 1, 2])
toJSON(x[yhat == 1, 3])
toJSON(x[yhat != 1, 3])
## calculate linear coefficients
slope <- -theta_star[2] / theta_star[3]
intercept <- -theta_star[1] / theta_star[3]
toJSON(py)
toJSON(py[1,])
toJSON(unlist(py))
class(py)
py[1,]
py[,1]
toJSON(py[,1])
toJSON(round(py[,1], 4))
toJSON(x[-train_ind, ] %*% theta_star)
py_x <- x[-train_ind, ] %*% theta_star
toJSON(py_x[,1])
a <- py[,1] == 1
toJSON(round(py[a,1], 4))
a <- py[,1] <= 0.5
toJSON(round(py[a,1], 4))
a <- py[,1] < 0.5
toJSON(round(py[a,1], 4))
a <- py[,1] < 0.5
toJSON(round(py[a,1], 4))
py_x <- x[-train_ind, ] %*% theta_star
toJSON(py_x[a,1])
toJSON(round(py[-a,1], 4))
toJSON(py_x[-a,1])
a <- py[,1] < 0.5
toJSON(round(py[-a,1], 4))
a <- py[, 1] >= 0.5
toJSON(round(py[-a, 1], 4))
a <- py[, 1] >= 0.5
toJSON(round(py[a, 1], 4))
py_x <- x[-train_ind, ] %*% theta_star
toJSON(py_x[a, 1])
toJSON(round(py[-a, 1], 4))
a <- py[, 1] < 0.5
toJSON(round(py[a, 1], 4))
toJSON(py_x[a, 1])
min(py_x)
max(py_x)
plot(iris$Petal.Width, iris$Petal.Length)
plot(iris$Petal.Length, iris$Petal.Width)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width, Petal.Width^2, Petal.Length^2) %>% as.matrix
## logistic function
g <- function(z) 1 / (1 + exp(-z))
## output logistic plot
library(jsonlite)
z <- seq(-5, 5, 0.1)
toJSON(g(z))
toJSON(z)
## vectorized logistic gradient function
logistic.gradient <- function(x, y, theta){
t(x) %*% (1 / (1 + exp(-(x %*% theta))) - y)
}
## gradient descent closure
gradient.descent <- function(gradient){
grad <- gradient
fit <- function(x, y, theta = NULL, maxiter = 10000, alpha = 0.05, tol = .00001){
x <- as.matrix(x)
y <- as.matrix(y)
m <- ncol(x)
if(is.null(theta)) theta <- matrix(0, nrow = m, 1)
for(i in 1:maxiter){
theta.tmp <- theta
theta <- theta.tmp - alpha * grad(x, y, theta.tmp)
}
return(
list(theta = theta)
)
}
return(fit)
}
## initialize a gradient descent closure with the logistic gradient
mdl <- gradient.descent(logistic.gradient)
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
## fit theta to the training set
theta_star <- mdl(x[train_ind, ], y[train_ind, ])$theta
## generate predictions using the best fit theta
py <- g(x[-train_ind, ] %*% theta_star)
## apply decision rule to assign to 0,1 label
yhat <- ifelse(py < 0.5, 0, 1)
## test to see how well the data fits
sum(yhat == y[-train_ind, ]) / length(y[-train_ind, ])
## check out the coefficients
theta_star
toJSON(x[yhat == 1, 2])
toJSON(x[yhat != 1, 2])
toJSON(x[yhat == 1, 3])
toJSON(x[yhat != 1, 3])
## calculate linear coefficients
slope <- -theta_star[2] / theta_star[3]
intercept <- -theta_star[1] / theta_star[3]
a <- py[, 1] < 0.5
toJSON(round(py[a, 1], 4))
py_x <- x[-train_ind, ] %*% theta_star
toJSON(py_x[a, 1])
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width, Petal.Width^2, Petal.Length^2) %>% as.matrix
plot(iris$Petal.Length, iris$Petal.Width)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width,
Petal.Width^2, Petal.Length^2) %>% as.matrix
names(x)
head(x)
x <- iris %>% mutate(Intercept = 1, Petal.Wsqr = Petal.Width^2,
Petal.Lsqr = Petal.Length^2) %>%
select(Intercept, Petal.Length, Petal.Width, Petal.Wsqr, Petal.Lsqr^2) %>% as.matrix
x <- iris %>% mutate(Intercept = 1, Petal.Wsqr = Petal.Width^2,
Petal.Lsqr = Petal.Length^2) %>%
select(Intercept, Petal.Length, Petal.Width, Petal.Wsqr, Petal.Lsqr) %>% as.matrix
theta_star <- mdl(x, y)$theta
theta_star
library(dplyr)
train_ind <- sample(seq_len(nrow(iris)), size = 100)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
theta_star <- mdl(x[train_ind, ], y[train_ind, ])$theta
py <- g(x[-train_ind, ] %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y[-train_ind, ]) / length(y[-train_ind, ])
theta_star
toJSON(x[yhat == 1, 2])
toJSON(x[yhat != 1, 2])
toJSON(x[yhat == 1, 3])
toJSON(x[yhat != 1, 3])
slope <- -theta_star[2] / theta_star[3]
intercept <- -theta_star[1] / theta_star[3]
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y[-train_ind, ]) / length(y[-train_ind, ])
x %>% select(2)
x %>% as.data.frame %>% select(Petal.Length)
slice(-train_ind)
filter(-train_ind)
g <- function(z) 1 / (1 + exp(-z))
## output logistic plot
library(jsonlite)
z <- seq(-5, 5, 0.1)
toJSON(g(z))
toJSON(z)
## vectorized logistic gradient function
logistic.gradient <- function(x, y, theta){
t(x) %*% (1 / (1 + exp(-(x %*% theta))) - y)
}
## gradient descent closure
gradient.descent <- function(gradient){
grad <- gradient
fit <- function(x, y, theta = NULL, maxiter = 10000, alpha = 0.05, tol = .00001){
x <- as.matrix(x)
y <- as.matrix(y)
m <- ncol(x)
if(is.null(theta)) theta <- matrix(0, nrow = m, 1)
for(i in 1:maxiter){
theta.tmp <- theta
theta <- theta.tmp - alpha * grad(x, y, theta.tmp)
}
return(
list(theta = theta)
)
}
return(fit)
}
## initialize a gradient descent closure with the logistic gradient
mdl <- gradient.descent(logistic.gradient)
library(dplyr)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Petal.Length, Petal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
## fit theta to the training set
theta_star <- mdl(x,y)$theta
py <- g(x %*% theta_star)
yhat <- ifelse(py < 0.5, 0, 1)
sum(yhat == y) / length(y)
theta_star
toJSON(x[yhat == 1, 2])
toJSON(x[yhat != 1, 2])
toJSON(x[yhat == 1, 3])
toJSON(x[yhat != 1, 3])
slope <- -theta_star[2] / theta_star[3]
intercept <- -theta_star[1] / theta_star[3]
intercept
intercept + slope * 1.2
intercept + slope * 1.5
intercept + slope * 2
intercept + slope * 2.1
intercept + slope * 2.05
intercept + slope * 2.01
intercept + slope * 2.03
intercept + slope * 2.05
intercept + slope * 2.075
a <- py[, 1] < 0.5
toJSON(round(py[a, 1], 4))
toJSON(py_x[a, 1])
py_x <- x %*% theta_star
toJSON(py_x[a, 1])
a <- py[, 1] >= 0.5
toJSON(round(py[a, 1], 4))
toJSON(py_x[a, 1])
theta_star
x <- iris %>% mutate(Intercept = 1, Petal.Wsqr = Petal.Width^2,
Petal.Lsqr = Petal.Length^2) %>%
select(Intercept, Petal.Length, Petal.Width, Petal.Wsqr, Petal.Lsqr) %>% as.matrix
theta_star <- mdl(x, y)$theta
theta_star
## initialize a gradient descent closure with the logistic gradient
mdl <- gradient.descent(logistic.gradient)
library(dplyr)
x <- iris %>% mutate(Intercept = 1) %>%
select(Intercept, Sepal.Length, Sepal.Width) %>% as.matrix
y <- iris %>% mutate(Setosa = as.integer(Species == 'setosa')) %>%
select(Setosa) %>% as.matrix
## fit theta to the training set
theta_star <- mdl(x,y)$theta
## generate predictions using the best fit theta
py <- g(x %*% theta_star)
## apply decision rule to assign to 0,1 label
yhat <- ifelse(py < 0.5, 0, 1)
## test to see how well the data fits
sum(yhat == y) / length(y)
## check out the coefficients
theta_star
py_x <- x %*% theta_star
a <- py[, 1] >= 0.5
toJSON(round(py[a, 1], 4))
py_x <- x[-train_ind, ] %*% theta_star
toJSON(py_x[a, 1])
a <- py[, 1] < 0.5
toJSON(round(py[a, 1], 4))
py_x <- x[-train_ind, ] %*% theta_star
toJSON(py_x[a, 1])
a <- py[, 1] < 0.5
toJSON(round(py[a, 1], 4))
py_x <- x %*% theta_star
toJSON(py_x[a, 1])
a <- py[, 1] >= 0.5
toJSON(round(py[a, 1], 4))
py_x <- x %*% theta_star
toJSON(py_x[a, 1])
